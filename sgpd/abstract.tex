Gaussian processes (GPs) with derivatives are useful in many applications,
including Bayesian optimization, implicit surface reconstruction, and terrain
reconstruction. Fitting a GP to function values and derivatives at $n$ points in
$d$ dimensions requires linear solves and log determinants with an ${n(d+1)
\times n(d+1)}$ positive definite matrix -- leading to prohibitive $\calO
(n^3d^3)$ computationsfor standard direct methods. We propose iterative solvers
using fast $\calO(nd)$ matrix-vector multiplications (MVMs), together with
pivoted Cholesky preconditioning that cuts the iterations to convergence by
several orders of magnitude, allowing for fast kernel learning and prediction.
Our approaches, together  with dimensionality reduction, allows us to scale
Bayesian optimization with derivatives to high-dimensional problems and large
evaluation budgets.