Understanding the low\hyp{}dimensional geometry of noisy and complex data is a
fundamental problem of unsupervised learning. Probabilistic models explain data
generation processes in terms of low\hyp{}dimensional latent variables. 
Inferring a posterior distribution for these latent variables provides us with a
compact representation for various exploratory analyses and downstream tasks.
However, exact inference is often intractable due to entangled interactions
between the latent variables \cite{LDA,airoldi2008mixed,erosheva2003bayesian,
pritchard2000inference}. Variational inference transforms the posterior
approximation into an optimization problem over simpler distributions with
independent parameters \cite{JGJS,WJ,blei2017variational}, while Markov Chain
Monte Carlo enables users to sample from the desired posterior distribution 
\cite{neal1993probabilistic,neal2011mcmc,robert2013monte}. However, these
likelihood-based methods require numerous iterations without any guarantee
beyond local improvement at each step \cite{kulesza2014low}.

When the data consists of collections of discrete objects, co\hyp{}occurrence
statistics summarize interactions between objects. Collaborative filtering
learns low\hyp{}dimensional representations of individual items, which are
useful for recommendation systems, by explicitly decomposing the 
co\hyp{}occurrence of items that are jointly consumed by certain users 
\cite{moontae2015nips,liang2016factorization}. Word\hyp{}vector models learn 
low\hyp{}dimensional embeddings of individual words, which encode useful
linguistic biases for neural networks, by implicitly decomposing the 
co\hyp{}occurrence of words that appear together 
\cite{pennington2014glove,levy2014neural}. If co\hyp{}occurrence provides a rich
enough set of unbiased moments about an underlying generative model, spectral
methods can provably learn posterior configurations from co\hyp{}occurrence
information alone, without iterating through individual training examples 
\cite{arora2013practical,anandkumar2012method,hsu2012spectral,AHK}.

However, two major limitations hinder users from taking advantage of spectral
inference based on co\hyp{}occurrence. First, the second\hyp{}order 
co\hyp{}occurrence matrix grows quadratically in the size of the objects (e.g.
items, words, products). Pruning these objects is an option, but for a retailer
selling millions of products, a low\hyp{}dimensional representation of a small
subset of the products is inadequate. Second, inference quality is poor in real
data that does not necessarily follow our computational model. Whereas
likelihood\hyp{}based methods have an intrinsic capability to fit the data to
the model despite their mismatch, sample noise can destroy the  performance of
spectral methods even if the data is synthesized from the model 
\cite{kulesza2014low}. {\em Rectification}, a process of projecting empirical 
co\hyp{}occurrence onto a set consistent with the geometry of the model, can
improve the performance of spectral inference in the face of model mismatch 
\cite{moontae2015nips}. But running multiple projections dominates overall
inference cost even when the vocabulary is small. In addition, the rectification
process makes the co\hyp{}occurrence dense, exacerbating storage costs when
dealing with large vocabularies.

In this chapter, we propose the Epsilon Non\hyp{}Negative rectification (ENN)
and the Low\hyp{}rank Anchor Word algorithm (LAW). Given a vocabulary of $N$
objects and the user\hyp{}specified latent dimension $K$, ENN simultaneously
compresses and rectifies the co\hyp{}occurrence matrix $C \in \BBR^{N \Times
N}$ into $YY^T$ with $Y \in \BBR^{N \Times K}$. Each entry of the decompression
$(YY^T)_{ij}$ tightly approximates the rectified co\hyp{}occurrence $C^*_{ij}$ 
but is allowed to be a tiny negative value above $-\epsilon$. Then LAW learns
the latent clusters (e.g., topics of documents or genres of items) and their
correlations provided only with $Y$, guaranteeing the same performance as
running the original Anchor Word algorithm on $C^*$ if $YY^T \geq 0$. In
contrast, we also formulate the Proximal Alternating Linearized Minimization 
rectification algorithm (PALM) that approximates the rectified 
co\hyp{}occurrence $C^*$ by $YY^T$ with $Y \geq 0$. While the
non\hyp{}negativity of $Y$ in the PALM approach allows LAW to perform exact
inference, the fact that PALM enforces more constraints than ENN means that PALM
also provides a less faithful approximation to the original co\hyp{}occurrence
data. Our experiments on various textual and non\hyp{}textual datasets show that
ENN learns a high\hyp{}quality factor $Y$ for which LAW provides results of
quality comparable to those based on the full co\hyp{}occurrence $C^*$; in
contrast, while PALM works comparably in some settings, in others there is a
visible loss of accuracy.

We also adopt a randomized algorithm that constructs a low\hyp{}rank
approximation of the full co\hyp{}occurrence $C$ directly from the raw data.
While PALM requires the full co\hyp{}occurrence, ENN can work directly with the
low\hyp{}rank initialization, eliminating the need to ever store a full 
co\hyp{}occurrence matrix. Note also that the second\hyp{}order methods rely on
the \textit{separability assumption},\footnote{Each cluster has at least one
anchor object which dedicates exclusively to that cluster, nothing else.} which
has been another criticism in theory despite their superior performance in
practice \cite{moontae2017from}. Our analyses show that models based on large
vocabularies find more separable anchor objects, learning stable latent clusters
without much sensitivity to sample noise. Overall, we complete a robust and
scalable pipeline: that efficiently performs quality posterior inference for
unsupervised learning from co\hyp{}occurrence information within time and space
complexity linear in $N$. 

The major contributions of our paper are:
\begin{itemize}[leftmargin=*]
    \item We introduce two efficient rectifications, ENN and PALM, that compress
    quadratic and noisy co\hyp{}occurrence information on the fly with a linear
    space rectified representation. 
    \item We develop a low\hyp{}rank algorithm (LAW) for 
    anchor\hyp{}word\hyp{}based topic modeling that works directly on the
    compressed rectified representations and provides near\hyp{}exact
    performance.
    \item We propose a robust and scalable pipeline, LR\hyp{}JSMF, that learns
    topic models with a small number of passes directly over the data.  This new
    pipeline scales to large vocabularies that were previously intractable for
    spectral inference, and offers a $\mathbf{\sim}100\Times$ speedup over
    previous methods for general data sets.
\end{itemize} 