Across many data domains, co\hyp{}occurrence statistics about the joint
appearance of objects are powerfully informative. By transforming unsupervised
learning problems into decompositions of co\hyp{}occurrence statistics, spectral
inference provides transparent algorithms and optimality guarantees for
non-linear dimensionality reduction or latent topic analysis. As object
vocabularies grow, however, it becomes rapidly more expensive to store and run
inference algorithms on co\hyp{}occurrence statistics. Current rectification
techniques, which preprocess real data in order to overcome model-data mismatch,
are even more expensive, as they require iterative projections that destroy
sparsity of the co\hyp{}occurrence data. In this paper, we propose novel
approaches that can simultaneously compress and rectify the co\hyp{}occurrence
statistics, scaling gracefully with the size of vocabulary and the dimension of
latent space. We also present new algorithms that are capable of learning latent
variables from the compressed statistics without losing visible precision, and
verify that they perform comparably to previous approaches on both textual and
non-textual data.