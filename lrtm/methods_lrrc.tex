The rectified co-occurrence $C^*$ in \cref{lrtmsec:bac} must be of rank $K$ and
positive semidefinite, hinting at an opportunity to represent it in terms of an
outer product $Y Y^T$ for some $Y \in \BBR^{N \Times K}$. One idea for achieving
this structure is to use a low\hyp{}rank representation $C_t = Y_t Y_t^T$
throughout the rectification in \cref{alg:rawa}. Another way to obtain this
structure is to directly minimize $\norm{C - Y Y^T}_F$ with the necessary
constraints. In this section, we propose two new algorithms to simultaneously
compress and rectify the input by representing $C \in \BBR^{N \Times N}$ by a
low\hyp{}rank outer product $Y Y^T$.
\begin{figure}[ht]
	\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		\SetKwInput{KwOutput}{Output}
		\SetKwRepeat{Repeat}{repeat $with \; t=0, 1, 2, ...$}{until}
		\DontPrintSemicolon
		\KwInput{Object co-occurrence $C \in \BBR^{N \Times N}$}
		\hspace{28px} The number of clusters $K$\\
		\KwOutput{Rectified compression $Y \in \BBR^{N \Times K}$}
		\Begin{
			$E \leftarrow \bm{0}\in\BBR^{N\times N}\;\;$ (sparse format)\;
			$C^{op}: x \rightarrow C x\;\;$ (Implicit operator)\;
			\Repeat{$E$ converges}{
				$(U,\Lambda_K) \leftarrow$ Truncated-Eig($C^{op}, K$)\;
				\vspace{2px}
				$\Lambda_K^+ \leftarrow \max(\Lambda_K, 0)$\;
				\vspace{2px}
				$Y \leftarrow U(\Lambda_K^+)^{1/2}$\;
				\vspace{2px}
				$E_{ij} \leftarrow \max(-Y_{i\ast}Y_{j\ast}^T, 0)$\;
				\vspace{2px}
				$r \leftarrow (1 - \norm{Y^T\Be}_2^2-\sum_{ij}E_{ij})/N^2$\;
				\vspace{1px}
				$C^{op}:x \rightarrow Y(Y^Tx) + Ex + r(e^Tx)e$\;
			}
		}
		\caption{ENN-rectification (ENN)}
	\label{alg:enn}        
	\end{algorithm}
\end{figure}

\begin{figure}[ht]
  \begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		\SetKwInput{KwOutput}{Output}
		\DontPrintSemicolon
		\KwInput{Object co-occurrence $C \in \BBR^{N \Times N}$}
		\hspace{28px} The number of clusters $K$\\
		\KwOutput{Rectified compression $Y \in \BBR^{N \Times K}$}
		\Begin{
			$(V, D) \leftarrow$ Truncated-Eig$(C, K)$\;
			$(X_0, Y_0) \leftarrow (V \sqrt{D}, V \sqrt{D})$\;
	    \Repeat{$Y$ converges}{
	    	$c_t \leftarrow \gamma L_1(Y_{t})$\;
	      \vspace{3px}
	      $X_{t+1}' \leftarrow X_{t} - (1/c_t)\nabla_{X} J(X_{t}, Y_{t})$\;
	      \vspace{3px}
	      $X_{t+1} \leftarrow \max(X_{t+1}', 0)$\;
	      \vspace{3px}
	      $d_t \leftarrow \gamma L_2(X_{t+1})$\;
	      \vspace{3px}
	      $Y_{t+1}' \leftarrow Y_{t} - (1/d_t)\nabla_{Y} J(X_{t+1}, Y_{t})$\;
	      \vspace{3px}
	      $Y_{t+1} \leftarrow \max(Y_{t+1}', 0)$\;
	    }
		}
		\caption{PALM-rectification (PALM)}
	  \label{alg:palm}      
	\end{algorithm}
\end{figure}

\subsection{Epsilon Non-Negative Rectification}
The alternating projection iteration in \cref{alg:rawa} produces
low\hyp{}rank semi\hyp{}definite intermediate matrices in factored form at each
iteration. By construction, the positive semi\hyp{}definite projection 
($\PSD_K$) and the normalization projection ($\NOR$) produce positive
semi\hyp{}definite matrices of rank $K$ and $K+1$, respectively.  Unfortunately,
the final projection to enforce elementwise non-negativity ($\NN$) destroys this
low rank structure. However, the $\NN$ projection only makes significant changes
to a few elements; that is, the output of the $\NN$ projection at step $t$ is
nearly rank $K+1$ plus a sparse correction $E_t$.  The Epsilon Non\hyp{}Negative
Rectification algorithm (\cref{alg:enn}) has the same structure as 
\cref{alg:rawa}, but with the key difference that it returns a
sparse\hyp{}plus\hyp{}low\hyp{}rank representation of the $\NN$ projection
rather than materializing a dense representation. Matrix\hyp{}vector products
with this sparse\hyp{}plus\hyp{}low\hyp{}rank representation require $\calO(NK + 
\operatorname{nnz}(E_t))$ time, and $\calO(K)$ such matrix\hyp{}vector products
can be used in a Lanczos eigensolver to compute the truncated eigendecomposition
at the start of the next iteration.

Maintaining a sparse correction matrix $E_t$ at each step lets the ENN approach
avoid the storage overheads of the original alternating projection algorithm. 
To overcome the quadratic time cost at each iteration, though, we need to avoid
explicitly computing every element of the intermediate $Y Y^T$ in the course of
the $\NN$ projection. However, we can bound the magnitude of many elements of
$YY^T$ by the Cauchy\hyp{}Schwartz inequality: $\abs{C_{ij}} \leq \norm{y_i}_2 
\norm{y_j}_2$ where $y_i$ and $y_j$ denote columns of $Y^T$. Let $I$ denote the
index set indices $\{i :\norm{y_i}^2_2 > \epsilon\} \subseteq [N]$ for given
$\epsilon$; then every large entry of $C$ belongs to either $Y_{I*}Y^T$ or $Y
(Y^T)_{*I}$. As $C$ is symmetric, checking the negative entries in $Y_{I*}Y^T$
is sufficient to find a symmetric correction $E$ that guarantees $YY^T \Plus E
\geq -\epsilon$. We refer to this property as \textit{Epsilon
Non\hyp{}Negativity}: $\epsilon$ balances the trade\hyp{}off between the effect
of leaving small negative entries versus increasing the size of $I$ to look up.
We limit $\abs{I}$ to be $\calO(K)$ based on the common sampling complexity of a
suitable set of rows for a near\hyp{}optimal rank\hyp{}K approximation
\footnote{This choice is standard in literature on low\hyp{}rank approximation
via column subset selection.}.

\subsection{Proximal Alternating Linearized Minimization Rectification}

To avoid small negative entries, we investigate another rectified compression
algorithm that directly minimizes $\norm{C - Y Y^T}_F$ subject to the stronger
$\NN$\hyp{}constraint $Y \geq 0$ and the usual $\NOR$\hyp{}constraint
$\norm{Y^Te}_2 \Eq 1$. Concretely, we try to
\begin{equation}
  \text{minimize} \;\; J(X, Y) := \frac{1}{2} \norm{C - XY^T}_F^2 + \frac{s}{2}
  \norm{X - Y}_F^2 \;\;\;\; \text{subject to} \;\; X \geq 0, Y \geq 0\,. 
  \label{eqn:palm}
\end{equation}
$\PSD_K$- and $\NOR$-constraints are implicitly satisfied by jointly minimizing
the two terms in the objective function $J$, whereas $\NN$\hyp{}constraint is
explicit in the formulation. Thus we can apply the Proximal Alternating
Linearized Minimization (PALM)\cite{bolte2014proximal} for learning $Y$ given
$C$; the relevant proximal operator is $\NN$ projection of $Y$, which takes
$\calO(NK)$ at most.

Note that $J$ is semi\hyp{}algebraic (as it is a real polynomial) with two
partial derivatives: $\nabla_{X}J \Eq (X Y^T \Minus C)Y \Plus s(X\Minus Y)$ and
$\nabla_{Y}J \Eq (Y X^T \Minus C)X \Plus s(Y \Minus X)$. So, the following lemma
guarantees the global convergence.
\begin{lemma}
  For any fixed $Y$, $\nabla_{X}J(X, Y)$ is globally Lipschitz continuous with
  the moduli $L_1(Y) \Eq \norm{Y^T Y \Plus sI_K}_2$. So is $\nabla_{Y}J(X, Y)$
  given any fixed $X$ with $L_2(X) \Eq \norm{X^T X \Plus sI_K}_2$.
\end{lemma}
\begingroup
\allowdisplaybreaks
\begin{proof}
	\begin{align*}
		&\norm{\nabla_{X}J(X, Y) - \nabla_{X}J(X', Y)}_F \\
		\Eq{} & \norm{(Y^TY + sI_K)(X - X')}_F \\
		\leq{} & \norm{Y^T Y + sI_k}_2 \cdot\norm{X - X'}_F
	\end{align*} 
	The proof is symmetric for the other case with $L_2(X) = \norm{X^TX+ sI_K}_2$.
\end{proof}
\endgroup
\cref{alg:palm} shows the PALM\hyp{}rectification with the adaptive control of
the learning rates based on the tight 2\hyp{}norm Lipschitz modulis at each step
$t$.