There is a pressing need for scalable machine learning approaches to extract
rich statistical structure from large datasets. A common bottleneck --- arising
in determinantal point processes \cite{kulesza2012determinantal}, Bayesian
neural networks \cite{mackay1992bayesian}, model comparison 
\cite{mackay2003information}, graphical models \cite{rue2005gaussian}, and
Gaussian process kernel learning \cite{rasmussen06} --- is computing a log
determinant over a large positive definite matrix. While we can approximate log
determinants by existing stochastic expansions relying on matrix\hyp{}vector
multiplications (MVMs), these approaches make assumptions, such as
near\hyp{}uniform eigenspectra \cite{boutsidis2015randomized}, which are
unsuitable in machine learning contexts. For example, the popular RBF kernel
gives rise to rapidly decaying eigenvalues. Moreover, while standard approaches,
such as stochastic power series, have reasonable asymptotic complexity in the
rank of the matrix, they require too many terms (MVMs) for the precision
necessary in machine learning applications.

Gaussian processes (GPs) provide a principled probabilistic kernel learning
framework, for which a log determinant is of foundational importance.
Specifically, the \emph{marginal likelihood} of a Gaussian process is the
probability of data given only kernel hyper\hyp{}parameters. This utility
function for kernel learning compartmentalizes into automatically calibrated
model fit and complexity terms --- called \emph{automatic Occam's razor} ---
such that the simplest models which explain the data are automatically favored 
\cite{rasmussen01, rasmussen06}, without the need for approaches such as
cross\hyp{}validation, or regularization, which can be costly, heuristic, and
involve substantial hand\hyp{}tuning and human intervention. The automatic
complexity penalty, called the \emph{Occam's factor} 
\cite{mackay2003information}, is a log determinant of a kernel (covariance)
matrix, related to the volume of solutions that can be expressed by the Gaussian
process. Unfortunately, calculating log determinant is usually very expensive
for huge matrices. The exact kernel learning costs of $\calO (N^3)$ flops and
the prediction cost of $\calO(N)$ flops per test point are clearly
computationally infeasible for large datasets. As a result, GPs have
traditionally been limited to a few thousand data points.

In addition, derivative information greatly enhances the performance of GPs in
many applications, including Bayesian Optimization (BO) \citep{wu2017bayesian},
implicit surface reconstruction \citep{macedo2011hermite}, and terrain
reconstruction.  For many simulation models, derivatives may be computed at
little extra cost via finite differences, complex step approximation, an adjoint
method, or algorithmic differentiation \citep{forrester2008engineering}. Hence,
there are ample opportunities in learning better GP models through exploiting
derivative information in practice. However, the computation becomes more
challenging if we consider GPs with both function value and derivative
information, in which case training and prediction become $\calO(N^3d^3)$ and
$\calO(Nd)$ respectively for data points in $d$\hyp{}dimensional space
\citep[\S9.4]{rasmussen06}.

Many current approaches to scalable Gaussian processes \cite[e.g.,][]
{quinonero2005unifying,le2013fastfood,hensman2013uai} focus on inference
assuming a fixed kernel, or use approximations that do not allow for very
flexible kernel learning \cite{wilson2014thesis}, due to poor scaling with
number of basis functions or inducing points. Alternatively, approaches which
exploit algebraic structure in kernel matrices can provide highly expressive
kernel learning \cite{wilson2014fast}, but are essentially limited to grid
structured data. On the other hand, while many scalable approximation methods
for Gaussian process regression have been proposed, scalable methods
incorporating derivatives have received little attention.

Recently, \citet{wilson2015kernel} proposed the {\em structured kernel
interpolation} (SKI) framework, which generalizes structuring exploiting methods
to arbitrarily located data. SKI works by providing accurate and fast
matrix\hyp{}vector multiplies (MVMs) with kernel matrices, which can then be
used in iterative solvers such as linear conjugate gradients for scalable GP
inference. However, evaluating the marginal likelihood and its derivatives, for
kernel learning, has followed a scaled eigenvalue approach 
\citep{wilson2014fast,wilson2015kernel} instead of iterative MVM approaches.
This approach can be inaccurate, and relies on a fast eigendecomposition of a
structured matrix, which is not available in many consequential situations where
fast MVMs are available, including: (i) additive covariance functions, (ii)
multi\hyp{}task learning, (iii) change\hyp{}points \citep{herlands2016scalable},
and (iv) diagonal corrections to kernel approximations 
\citep{snelson2006sparse}. Fiedler \citep{fiedler84hankelLoewner} and Weyl 
\citep{weyl1912} bounds have been used to extend the scaled eigenvalue approach 
\citep{flaxman2015fast,herlands2016scalable}, but are similarly limited. These
extensions are often very approximate, and do not apply beyond sums of two and
three matrices, where each matrix in the sum must have a fast
eigendecomposition.

In machine learning there has recently been renewed interest in MVM based
approaches to approximating log determinants, such as the Chebyshev 
\citep{han2015large} and Lanczos\citep{ubarufast} based methods, although these
approaches go back at least two decades in quantum chemistry computations 
\citep{bai1998computing}. Independently, several authors have proposed various
methods to compute derivatives of log determinants~\citep{mackay1997efficient,
stein2013stochastic}. But {\em both} the log determinant {\em and} the
derivatives are needed for efficient GP marginal likelihood learning: the
derivatives are required for gradient\hyp{}based optimization, while the log
determinant itself is needed for model comparison, comparisons between the
likelihoods at local maximizers, and fast and effective choices of starting
points and step sizes in a gradient\hyp{}based optimization algorithm.

In this chapter, we develop novel scalable and general purpose Chebyshev,
Lanczos, and surrogate approaches for efficiently and accurately computing both
the log determinant and its derivatives simultaneously. Our methods use only
fast MVMs, and re\hyp{}use the same MVMs for both computations. We also propose
scalable methods for GPs with derivative information built on the the SKI
framework. As the uniform grids in SKI scale poorly to high\hyp{}dimensional
spaces, we further extend the structured kernel interpolation for products 
(SKIP) method, which approximates a high\hyp{}dimensional product kernel as a
Hadamard product of low rank Lanczos decompositions \citep{gardner2018product}.
Both SKI and SKIP provide fast approximate kernel MVMs, which are a building
block to solve linear systems with the kernel matrix and to approximate log
determinants~\citep{dong2017scalable}. In particular, our contributions are:
\begin{itemize}
  \item We derive fast methods for simultaneously computing the log determinant
  and its derivatives by stochastic Chebyshev, stochastic Lanczos, and surrogate
  models, from MVMs alone. We also perform an error analysis and extend these
  approaches to higher order derivatives.

  \item These methods enable fast GP kernel learning whenever fast MVMs are
  possible, including applications where alternatives such as scaled eigenvalue
  methods (which rely on fast eigendecompositions) are not, such as for (i)
  diagonal corrections for better kernel approximations, (ii) additive
  covariances, (iii) multi\hyp{}task approaches, and (iv) non\hyp{}Gaussian
  likelihoods.

  \item We extend SKI to incorporate derivative information, enabling $\calO
  (Nd)$ complexity learning and $\calO(1)$ prediction per test points, relying
  only on fast MVM with the kernel matrix.

  \item We also extend SKIP, which enables scalable Gaussian process regression
  with derivatives in high\hyp{}dimensional spaces without grids. Our approach
  allows for $\calO(Nd)$ MVMs after the inclusion of derivative information.

  \item We illustrate that preconditioning is critical for fast convergence of
  iterations for kernel matrices with derivatives. A pivoted Cholesky
  preconditioner cuts the iterations to convergence by several orders of
  magnitude when applied to both SKI and SKIP with derivatives.

  \item For GPs without derivative information, we illustrate the performance of
  our approach on several large, multi\hyp{}dimensional datasets, including a
  consequential crime prediction problem, and a precipitation problem with $N =
  528,474$ training points. We consider a variety of kernels, including deep
  kernels \citep{wilson2016deep}, diagonal corrections, and both Gaussian and
  non\hyp{}Gaussian likelihoods.

  \item For GPs with derivative information, we illustrate the scalability of
  our approach on several examples including implicit surface fitting of the
  Stanford bunny, rough terrain reconstruction, and Bayesian optimization. We
  show how our methods, together with active subspace techniques, can be used to
  extend Bayesian optimization to high\hyp{}dimensional problems with large
  evaluation budgets.

  \item We have released code and tutorials as an extension to the GPML library 
  \citep{rasmussen10gpml} at \url{https://github.com/kd383/GPML_SLD}. A Python
  implementation of our approach is also available through the GPyTorch library:
  \url{https://github.com/jrg365/gpytorch}. The code for GPs with derivative
  information is available at 
  \url{https://github.com/ericlee0803/GP_Derivatives}.
\end{itemize}

When using our approach in conjunction with SKI \citep{wilson2015kernel} for
fast MVMs, derivative\hyp{}free GP kernel learning is $\mathcal{O}(N + g(M))$,
for $M$ inducing points and $N$ training points, where $g(M) \leq M \log M$.
With algebraic approaches such as SKI we also do not need to worry about
quadratic storage in inducing points, since symmetric Toeplitz and Kronecker
matrices can be stored with at most linear cost, without needing to explicitly
construct a matrix.

Although we here use SKI for fast MVMs, we emphasize that the proposed iterative
approaches are generally applicable, and can easily be used in conjunction with
\emph{any} method that admits fast MVMs, including classical inducing point
methods \citep{quinonero2005unifying}, finite basis expansions 
\citep{le2013fastfood}, and the popular stochastic variational approaches 
\citep{hensman2013uai}. Moreover, stochastic variational approaches can
naturally be combined with SKI to further accelerate MVMs 
\citep{wilson2016stochastic}.

We start in \cref{sgpsec:bac} with an introduction to GPs and kernel
approximations.  In \cref{sgpsec:met} we introduce stochastic trace
estimation, Chebyshev (\cref{sgpsec:che}) and Lanczos (\cref{sgpsec:lan})
approximations, and various techniques we use to efficiently evaluate log
determinant for kernel matrices. In \cref{sgpsec:err}, we describe the
different sources of error in our approximations. In \cref{sgpsec:dmet}, we
extend the methods in \cref{sgpsec:met} for GPs with derivative information. In
\cref{sgpsec:exp} we consider experiments for derivative\hyp{}free GPs on
several large real\hyp{}world data sets. In \cref{sgpsec:dexp}, we demonstrate
the benefit of including derivative information, as well as the performance of
our methods in this case. We conclude in \cref{sgpsec:con}.