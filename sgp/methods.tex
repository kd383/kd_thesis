Our goal is to estimate, for a symmetric positive definite matrix $\Ktil{}$,
\begin{equation}\label{eqn:ldnd}
  \log\abs{\Ktil{}} = \tr(\log(\Ktil{})) \quad \mbox{and} \quad \frac{\partial}
  {\partial \theta_i} \left[\log\abs{\Ktil{}}\right] = \tr\left( \Ktil{}^{-1}
  \left(\frac{\partial \Ktil{}}{\partial \theta_i} \right) \right),
\end{equation}
where $\log$ is the matrix logarithm \cite{higham2008functions}. We compute the
traces involved in both the log determinant and its derivative via {\em
stochastic trace estimators}~\cite{hutchinson1990stochastic}, which approximate
the trace of a matrix using only matrix vector products.

The key idea is that for a given matrix $A$ and a random probe vector $z$ with
independent entries with mean zero and variance one, then $\tr(A) = \BBE
[z^TAz]$; a common choice is to let the entries of the probe vectors be
Rademacher random variables. In practice, we estimate the trace by the sample
mean over $n_z$ independent probe vectors.  Often surprisingly few probe vectors
suffice.

To estimate $\tr(\log(\Ktil{}))$, we need to multiply $\log(\Ktil{})$ by probe
vectors. We consider two ways to estimate $\log(\Ktil{})z$: by a polynomial
approximation of $\log$ or by using the connection between the Gaussian
quadrature rule and the Lanczos method \cite{han2015large,ubarufast}. In both
cases, we show how to re-use the same probe vectors for an inexpensive coupled
estimator of the derivatives. In addition, we may use standard radial basis
function interpolation of the log determinant evaluated at a few systematically
chosen points in the hyperparameter space as an inexpensive surrogate for the
log determinant.

\subsection{Chebyshev Expansion}\label{sgpsec:che}

Chebyshev polynomials are defined by the recursion
\begin{equation}\label{eqn:chebrecursion}
  T_0(x) = 1, \quad T_1(x) = x, \quad T_{j+1}(x) = 2xT_j(x)-T_{j-1}(x)\; \text{
  for }\; j\geq 1.
\end{equation}
For $f : [-1,1] \to \BBR$ the Chebyshev interpolant of degree $m$ is
\begin{equation}\label{eqn:chebinterp}
  f(x) \approx p_m(x) := \sum_{j=0}^m c_j T_j(x), \quad \mbox{where } c_j = 
  \frac{2 - \delta_{j0}}{m+1} \displaystyle\sum_{k=0}^m f(x_k)T_j(x_k),
\end{equation}
and $\delta_{j0}$ is the Kronecker delta and $x_k = \cos(\pi(k+1/2)/(m+1))$
for $k=0,1,2,\ldots,m$; see~\cite{gil2007numerical}. Using the Chebyshev
interpolant of $\log(1+\alpha x)$, we approximate $\log\abs{\Ktil{}}$ by
\begin{equation}\label{eqn:sdescale}
  \log\abs{\Ktil{}} -n\log\beta = \log\abs{I+\alpha B} \approx \sum_{j=0}^m
  c_j\tr(T_j(B)),
\end{equation}
when $B=(\Ktil{}/\beta-1)/\alpha$ has eigenvalues $\lambda_i \in (-1,1)$.

For stochastic estimation of $\tr(T_j(B))$, we only need to compute $z^T T_j
(B)z$ for each given probe vector $z$. We compute vectors $w_j=T_j(B)z$ and
$\partial w_j/\partial \theta_i$ via the coupled recurrences
\begin{align}
  w_0 &= z, &
  w_1 &= Bz, &
  w_{j+1} &= 2Bw_j - w_{j-1} \mbox{ for } j \geq 1, \label{eqn:chebrec_w} \\
  \frac{\partial w_0}{\partial \theta_i} &= 0, &
  \frac{\partial w_1}{\partial \theta_i} &= \frac{\partial B}{\partial \theta_i}
  z, &
  \frac{\partial w_{j+1}}{\partial \theta_i} &=
  2\left(\frac{\partial B}{\partial \theta_i} w_j + B \frac{\partial w_j}
  {\partial \theta_i}\right) - \frac{\partial w_{j-1}}{\partial \theta_i} \mbox{
  for } j \geq 1.\label{eqn:chebrec_dw}
\end{align}
This gives the estimators
\begin{equation}\label{eqn:logdetexp}
  \log \abs{\tilde K} \approx \BBE\left[\sum_{j=0}^m c_j z^Tw_j\right] \quad 
  \mbox{
  and } \quad \frac{\partial}{\partial \theta_i} \log\abs{\tilde K} \approx 
  \BBE\left[\sum_{j=0}^m c_j z^T\frac{\partial w_j}{\partial \theta_i}\right].
\end{equation}
Thus, each derivative of the approximation costs two extra MVMs per term.

\subsection{Gauss Quadrature via Lanczos}\label{sgpsec:lan}

We can also approximate $z^T \log(\Ktil{}) z$ via a Lanczos decomposition; see~
\cite{golub2010matrices} for discussion of a Lanczos-based computation of $z^T f
(\Ktil{}) z$ and~\cite{ubarufast,bai1998computing} for stochastic Lanczos
estimation of log determinants.  We run $m$ steps of the Lanczos algorithm,
which computes the decomposition
\begin{equation}\label{eqn:Klanczos}
  \Ktil{} Q_m = Q_m T + \beta_m q_{m+1} e_m^T\,,
\end{equation}
where $Q_m = \left[q_1, q_2, \cdots, q_m\right] \In \BBR^{n \times m}$ is a
matrix with orthonormal columns such that $q_1 = z/\norm{z}$, $T \In \BBR^{m
\times m}$ is tridiagonal, $\beta_m$ is the residual, and $e_m$ is the $m$-th
Cartesian unit vector.  We estimate
\begin{equation} \label{eq:gaussq}
  z^T f(\Ktil{}) z \approx e_1^T f(\norm{z}^2 T) e_1\,,
\end{equation}
where $e_1$ is the first column of the identity. Because the Lanczos algorithm
is numerically unstable, there exist several practical implementations to
resolve this issue \cite{cullum2002lanczos,saad1992numerical}. The
approximation~\eqref{eq:gaussq} corresponds to a Gauss quadrature rule for the
Riemann-Stieltjes integral of the measure associated with the eigenvalue
distribution of $\Ktil{}$.  It is exact when $f$ is a polynomial of degree up
to $2m-1$.  This approximation is also exact when $\Ktil{}$ has at most $m$
distinct eigenvalues, which is particularly relevant to Gaussian process
regression, since frequently the kernel matrices only have a small
number of eigenvalues that are not close to zero.

The Lanczos decomposition also allows us to estimate derivatives of the log
determinant at minimal cost. Via the Lanczos decomposition, we have
\begin{equation}\label{eqn:lansolve}
  \hat{g} = Q_m (T^{-1} e_1 \norm{z}) \approx \Ktil{}^{-1}z \,.
\end{equation}
This approximation requires no additional matrix vector multiplications beyond
those used to compute the Lanczos decomposition, which we already used to
estimate $\log(\Ktil{}) z$; in exact arithmetic, this is equivalent to $m$ steps
of conjugate gradient (CG). Computing $\hat{g}$ in this way takes $\calO(mn)$
additional time; subsequently, we only need one matrix-vector multiply by
$\partial \Ktil{} / \partial \theta_i$ for each probe vector to estimate $\tr
(\Ktil{}^{-1} (\partial \Ktil{} / \partial \theta_i)) = \BBE\left[(\Ktil{}^{-1}
z)^T(\partial \Ktil{}/\partial \theta_i) z\right]$.


\subsection{Diagonal Correction to SKI}\label{sgpsec:dia}

The SKI approximation may provide a poor estimate of the diagonal entries of the
original kernel matrix for kernels with limited smoothness, such as the Mat\'ern
kernel. In general, diagonal corrections to scalable kernel approximations can
lead to great performance gains.  Indeed, the popular FITC method 
\citep{snelson2006sparse} is exactly a diagonal correction of SoR.

We thus modify the SKI approximation to add a diagonal matrix $D$,
\begin{equation}\label{eqn:skidiacor}
    \K{XX} \approx W \K{UU} W^T + D\, ,
\end{equation}
such that the diagonal of the approximated $\K{XX}$ is exact. In other words,
$D$ substracts the diagonal of $W \K{UU} W^T$ and adds the true diagonal of $K_
{XX}$.  This modification is not possible for the scaled eigenvalue method for
approximating log determinants in \citep{wilson2015kernel}, since adding a
diagonal matrix makes it impossible to approximate the eigenvalues of $\K{XX}$
from the eigenvalues of $\K{UU}$.

However, Eq.~\ref{eqn:skidiacor} still admits fast MVMs and thus works with our
approach for estimating the log determinant and its derivatives. Computing $D$
with SKI costs only $\calO(n)$ flops since $W$ is sparse for local cubic
interpolation. We can therefore compute $(W^Te_i)^T\K{UU}(W^Te_i)$ in $\calO(1)$
flops.

\subsection{Estimating higher derivatives}\label{sgpsec:hod}

We have already described how to use stochastic estimators to compute the log
marginal likelihood and its first derivatives. The same approach applies to
computing higher-order derivatives for a Newton-like iteration, to understand
the sensitivity of the maximum likelihood parameters, or for similar tasks. The
first derivatives of the full log marginal likelihood are
\begin{equation}\label{eqn:loglikderiv}
  \frac{\partial \calL}{\partial \theta_i} = -\frac{1}{2} \left[\tr\left(\Ktil
  {}^{-1} \frac{\partial \Ktil{}}{\partial \theta_i}\right) - \alpha^T 
  \frac{\partial \Ktil{}}{\partial \theta_i} \alpha\right]\,,
\end{equation}
and the second derivatives of the two terms are
\begin{align}
  \frac{\partial^2}{\partial \theta_i \partial \theta_j} \left[ \log |\tilde K|
  \right] &= \tr\left(\Ktil{}^{-1} \frac{\partial^2 \Ktil{}} {\partial \theta_i
  \partial \theta_j} - \Ktil{}^{-1} \frac{\partial \Ktil{}}{\partial \theta_i}
  \Ktil{}^{-1} \frac{\partial \Ktil{}}{\partial \theta_j}\right)\,, \\
  \frac{\partial^2}{\partial \theta_i \partial \theta_j} \left[ (y-\mu_X)^T
  \alpha \right] &= 2\alpha^T \frac{\partial \Ktil{}}{\partial \theta_i}
  \Ktil{}^{-1}\frac{\partial \tilde K}{\partial \theta_j} \alpha - \alpha^T 
  \frac{\partial^2 \Ktil{}} {\partial \theta_i \partial \theta_j}\alpha\,.
\end{align}
Superficially, evaluating the second derivatives would appear to require several
additional solves above and beyond those used to estimate the first derivatives
of the log determinant.  In fact, we can get an unbiased estimator for the
second derivatives with no additional solves, but only fast products with the
derivatives of the kernel matrices.  Let $z$ and $w$ be independent probe
vectors, and define $g = \Ktil{}^{-1}z$ and $h = \Ktil{}^{-1}w$.  Then
\begin{align}
  \frac{\partial^2}{\partial \theta_i \partial \theta_j} \left[\log\abs{\Ktil{}}
  \right] &= \BBE \left[ g^T \frac{\partial^2 \Ktil{}} {\partial \theta_i
  \partial \theta_j} z - \left( g^T \frac{\partial \Ktil{}}{\partial \theta_i}
  w \right) \left( h^T \frac{\partial \Ktil{}}{\partial \theta_j} z \right)
  \right]\,, \\
  \frac{\partial^2}{\partial \theta_i \partial \theta_j} \left[ (y-\mu_X)^T
  \alpha \right] &= 2 \BBE \left[\left( z^T \frac{\partial \Ktil{}}{\partial
  \theta_i} \alpha \right) \left( g^T \frac{\partial \Ktil{}}{\partial \theta_j}
  \alpha \right)\right] - \alpha^T \frac{\partial^2 \Ktil{}} {\partial \theta_i
  \partial \theta_j}\alpha\,.
\end{align}
Hence, if we use the stochastic Lanczos method to compute the log determinant
and its derivatives, the additional work required to obtain a second derivative
estimate is one MVM by each second partial of the kernel for each probe vector
and for $\alpha$, one MVM of each first partial of the kernel with $\alpha$, and
a few dot products.

\subsection{Radial Basis Functions}\label{sgpsec:rbf}
Another way to deal with the log determinant and its derivatives is to evaluate
the log determinant term at a few systematically chosen points in the space of
hyperparameters and fit an interpolation approximation to these values.  This
is particularly useful when the kernel depends on a modest number of
hyperparameters (e.g., half a dozen), and thus the number of points we need to
precompute is relatively small. We refer to this method as a surrogate, since it
provides an inexpensive substitute for the log determinant and its derivatives.
For our surrogate approach, we use radial basis function (RBF) interpolation
with a cubic kernel and a linear tail. See e.g.~\cite{buhmann2000radial,
fasshauer2007meshfree,schaback2006kernel,wendland2004scattered} and the
supplementary material for more details on RBF interpolation.