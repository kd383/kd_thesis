Our goal is to estimate, for a symmetric positive definite matrix $\Ktil{}$,
\begin{equation}\label{eqn:ldnd}
  \log\abs{\Ktil{}} = \tr(\log(\Ktil{})) \quad \mbox{and} \quad \frac{\partial}
  {\partial \theta_i} \left[\log\abs{\Ktil{}}\right] = \tr\left( \Ktil{}^{-1}
  \left(\frac{\partial \Ktil{}}{\partial \theta_i} \right) \right),
\end{equation}
where $\log$ is the matrix logarithm \cite{higham2008functions}. We compute the
traces involved in both the log determinant and its derivative via {\em
stochastic trace estimators}~\cite{hutchinson1990stochastic}, which approximate
the trace of a matrix using only matrix\hyp{}vector products.

The key idea is introduced in \cref{pre:ste}: for a given matrix $A$ and
a random probe vector $z$ with independent entries with mean zero and variance
one, then $\tr(A) = \BBE[z^TAz]$. In this section, we let the entries of the
probe vectors be Rademacher random variables. We also estimate the trace by the
sample mean over $N_z$ independent probe vectors. Often surprisingly few probe
vectors suffice.

To estimate $\tr(\log(\Ktil{}))$, we need to multiply $\log(\Ktil{})$ by probe
vectors. We consider two ways to estimate $\log(\Ktil{})z$: by a polynomial
approximation of $\log$ or by using the connection between the Gaussian
quadrature rule and the Lanczos method \cite{han2015large,ubarufast} 
(\cref{pre:fom}). In both cases, we show how to re-use the same probe vectors
for an inexpensive coupled estimator of the derivatives. In addition, we may use
standard radial basis function interpolation of the log determinant evaluated at
a few systematically chosen points in the hyper\hyp{}parameter space as an
inexpensive surrogate for the log determinant.

\subsection{Chebyshev Approximation}\label{sgpsec:che}

As given in \cref{eqn:cheb_3term}, Chebyshev polynomials are defined by the
recursion
\begin{equation}\label{eqn:chebrecursion}
  T_0(x) = 1, \quad T_1(x) = x, \quad T_{j+1}(x) = 2xT_j(x)-T_{j-1}(x)\; \text{
  for }\; j\geq 1\,.
\end{equation}
For $f : [-1,1] \to \BBR$ the Chebyshev interpolant of degree $m$ is
\begin{equation}\label{eqn:chebinterp}
  f(x) \approx p_m(x) := \sum_{j=0}^m c_j T_j(x)\,, \quad \mbox{where } c_j = 
  \frac{2 - \delta_{j0}}{m+1} \displaystyle\sum_{k=0}^m f(x_k)T_j(x_k)\,,
\end{equation}
and $\delta_{j0}$ is the Kronecker delta and $x_k = \cos(\pi(k+1/2)/(m+1))$
for $k=0,1,2,\ldots,m$; see~\cite{gil2007numerical}. Using the Chebyshev
interpolant of $\log(1+\alpha x)$, we approximate $\log\abs{\Ktil{}}$ by
\begin{equation}\label{eqn:sdescale}
  \log\abs{\Ktil{}} -n\log\beta = \log\abs{I+\alpha B} \approx \sum_{j=0}^m
  c_j\tr(T_j(B))\,,
\end{equation}
when $B=(\Ktil{}/\beta-1)/\alpha$ has eigenvalues $\lambda_i \in (-1,1)$.

For stochastic estimation of $\tr(T_j(B))$, we only need to compute $z^T T_j
(B)z$ for each given probe vector $z$. We compute vectors $w_j=T_j(B)z$ and
$\partial w_j/\partial \theta_i$ via the coupled recurrences
\begin{align}
  w_0 &= z\,, &
  w_1 &= Bz\,, &
  w_{j+1} &= 2Bw_j - w_{j-1} \mbox{ for } j \geq 1\,, \label{eqn:chebrec_w} \\
  \frac{\partial w_0}{\partial \theta_i} &= 0\,, &
  \frac{\partial w_1}{\partial \theta_i} &= \frac{\partial B}{\partial \theta_i}
  z\,, &
  \frac{\partial w_{j+1}}{\partial \theta_i} &=
  2\left(\frac{\partial B}{\partial \theta_i} w_j + B \frac{\partial w_j}
  {\partial \theta_i}\right) - \frac{\partial w_{j-1}}{\partial \theta_i} \mbox{
  for } j \geq 1\,.\label{eqn:chebrec_dw}
\end{align}
This gives the estimators
\begin{equation}\label{eqn:logdetexp}
  \log \abs{\tilde K} \approx \BBE\left[\sum_{j=0}^m c_j z^Tw_j\right] \quad 
  \mbox{
  and } \quad \frac{\partial}{\partial \theta_i} \log\abs{\tilde K} \approx 
  \BBE\left[\sum_{j=0}^m c_j z^T\frac{\partial w_j}{\partial \theta_i}\right]\,.
\end{equation}
Thus, each derivative of the approximation costs two extra MVMs per term.

\subsection{Gauss Quadrature and Lanczos}\label{sgpsec:lan}

We can also approximate $z^T \log(\Ktil{}) z$ via a Lanczos decomposition; see~
\cite{golub2010matrices} for discussion of a Lanczos\hyp{}based computation of
$z^T f(\Ktil{}) z$ and~\cite{ubarufast,bai1998computing} for stochastic Lanczos
estimation of log determinants.  We run $m$ steps of the Lanczos algorithm,
which computes the decomposition
\begin{equation}\label{eqn:Klanczos}
  \Ktil{} Q_m = Q_m\Gamma_m+ \beta_m q_{m+1} e_m^T\,,
\end{equation}
where $Q_m = \left[q_1, q_2, \cdots, q_m\right] \in \BBR^{n \times m}$ is a
matrix with orthonormal columns such that $q_1 = z/\norm{z}$, $\Gamma_m \in
\BBR^{m \times m}$ is tridiagonal, $\beta_m$ is the residual, and $e_m$ is the
$m$-th Cartesian unit vector.  We estimate
\begin{equation} \label{eq:gaussq}
  z^T f(\Ktil{}) z \approx e_1^T f(\norm{z}^2 \Gamma_m) e_1\,,
\end{equation}
where $e_1$ is the first column of the identity. Because the Lanczos algorithm
is numerically unstable, there exist several practical implementations to
resolve this issue \cite{cullum2002lanczos,saad1992numerical}. The
approximation~\eqref{eq:gaussq} corresponds to a Gauss quadrature rule for the
Riemann\hyp{}Stieltjes integral of the measure associated with the eigenvalue
distribution of $\Ktil{}$.  It is exact when $f$ is a polynomial of degree up
to $2m-1$.  This approximation is also exact when $\Ktil{}$ has at most $m$
distinct eigenvalues, which is particularly relevant to Gaussian process
regression, since frequently the kernel matrices only have a small
number of eigenvalues that are not close to zero.

The Lanczos decomposition also allows us to estimate derivatives of the log
determinant at minimal cost. Via the Lanczos decomposition, we have
\begin{equation}\label{eqn:lansolve}
  \hat{g} = Q_m (\Gamma_m^{-1} e_1 \norm{z}) \approx \Ktil{}^{-1}z \,.
\end{equation}
This approximation requires no additional matrix vector multiplications beyond
those used to compute the Lanczos decomposition, which we already used to
estimate $\log(\Ktil{}) z$; in exact arithmetic, this is equivalent to $m$ steps
of conjugate gradient (CG). Computing $\hat{g}$ in this way takes $\calO(mN)$
additional time; subsequently, we only need one matrix-vector multiply by
$\partial \Ktil{} / \partial \theta_i$ for each probe vector to estimate $\tr
(\Ktil{}^{-1} (\partial \Ktil{} / \partial \theta_i)) = \BBE\left[(\Ktil{}^{-1}
z)^T(\partial \Ktil{}/\partial \theta_i) z\right]$.


\subsection{Diagonal Correction to SKI}\label{sgpsec:dia}

The SKI approximation may provide a poor estimate of the diagonal entries of the
original kernel matrix for kernels with limited smoothness, such as the Mat\'ern
kernel. In general, diagonal corrections to scalable kernel approximations can
lead to great performance gains.  Indeed, the popular FITC method 
\citep{snelson2006sparse} is exactly a diagonal correction of SoR.

We thus modify the SKI approximation to add a diagonal matrix $D$,
\begin{equation}\label{eqn:skidiacor}
    \K{XX} \approx W \K{UU} W^T + D\, ,
\end{equation}
such that the diagonal of the approximated $\K{XX}$ is exact. In other words,
$D$ subtracts the diagonal of $W \K{UU} W^T$ and adds the true diagonal of $K_
{XX}$.  This modification is not possible for the scaled eigenvalue method for
approximating log determinants in \citep{wilson2015kernel}, since adding a
diagonal matrix makes it impossible to approximate the eigenvalues of $\K{XX}$
from the eigenvalues of $\K{UU}$.

However, Eq.~\ref{eqn:skidiacor} still admits fast MVMs and thus works with our
approach for estimating the log determinant and its derivatives. Computing $D$
with SKI costs only $\calO(n)$ flops since $W$ is sparse for local cubic
interpolation. We can therefore compute $(W^Te_i)^T\K{UU}(W^Te_i)$ in $\calO(1)$
flops.

\subsection{Estimating Higher Derivatives}\label{sgpsec:hod}

We have already described how to use stochastic estimators to compute the log
marginal likelihood and its first derivatives. The same approach applies to
computing higher\hyp{}order derivatives for a Newton\hyp{}like iteration, to
understand the sensitivity of the maximum likelihood parameters, or for similar
tasks. The first derivatives of the full log marginal likelihood are
\begin{equation}\label{eqn:loglikderiv}
  \frac{\partial \calL}{\partial \theta_i} = -\frac{1}{2} \left[\tr\left(\Ktil
  {}^{-1} \frac{\partial \Ktil{}}{\partial \theta_i}\right) - \alpha^T 
  \frac{\partial \Ktil{}}{\partial \theta_i} \alpha\right]\,,
\end{equation}
and the second derivatives of the two terms are
\begin{align}
  \frac{\partial^2}{\partial \theta_i \partial \theta_j} \left[ \log |\tilde K|
  \right] &= \tr\left(\Ktil{}^{-1} \frac{\partial^2 \Ktil{}} {\partial \theta_i
  \partial \theta_j} - \Ktil{}^{-1} \frac{\partial \Ktil{}}{\partial \theta_i}
  \Ktil{}^{-1} \frac{\partial \Ktil{}}{\partial \theta_j}\right)\,, \\
  \frac{\partial^2}{\partial \theta_i \partial \theta_j} \left[ (y-\mu_X)^T
  \alpha \right] &= 2\alpha^T \frac{\partial \Ktil{}}{\partial \theta_i}
  \Ktil{}^{-1}\frac{\partial \tilde K}{\partial \theta_j} \alpha - \alpha^T 
  \frac{\partial^2 \Ktil{}} {\partial \theta_i \partial \theta_j}\alpha\,.
\end{align}
Superficially, evaluating the second derivatives would appear to require several
additional solves above and beyond those used to estimate the first derivatives
of the log determinant.  In fact, we can get an unbiased estimator for the
second derivatives with no additional solves, but only fast products with the
derivatives of the kernel matrices.  Let $z$ and $w$ be independent probe
vectors, and define $g = \Ktil{}^{-1}z$ and $h = \Ktil{}^{-1}w$.  Then
\begin{align}
  \frac{\partial^2}{\partial \theta_i \partial \theta_j} \left[\log\abs{\Ktil{}}
  \right] &= \BBE \left[ g^T \frac{\partial^2 \Ktil{}} {\partial \theta_i
  \partial \theta_j} z - \left( g^T \frac{\partial \Ktil{}}{\partial \theta_i}
  w \right) \left( h^T \frac{\partial \Ktil{}}{\partial \theta_j} z \right)
  \right]\,, \\
  \frac{\partial^2}{\partial \theta_i \partial \theta_j} \left[ (y-\mu_X)^T
  \alpha \right] &= 2 \BBE \left[\left( z^T \frac{\partial \Ktil{}}{\partial
  \theta_i} \alpha \right) \left( g^T \frac{\partial \Ktil{}}{\partial \theta_j}
  \alpha \right)\right] - \alpha^T \frac{\partial^2 \Ktil{}} {\partial \theta_i
  \partial \theta_j}\alpha\,.
\end{align}
Hence, if we use the stochastic Lanczos method to compute the log determinant
and its derivatives, the additional work required to obtain a second derivative
estimate is one MVM by each second partial of the kernel for each probe vector
and for $\alpha$, one MVM of each first partial of the kernel with $\alpha$, and
a few dot products.

\subsection{Surrogate Model}\label{sgpsec:rbf}

Another way to deal with the log determinant and its derivatives is to evaluate
the log determinant term at a few systematically chosen points in the space of
hyperparameters and fit an interpolation approximation to these values.  This
is particularly useful when the kernel depends on a modest number of
hyperparameters (e.g., half a dozen), and thus the number of points we need to
precompute is relatively small. We refer to this method as a surrogate, since it
provides an inexpensive substitute for the log determinant and its derivatives.
For our surrogate approach, we use radial basis function (RBF) interpolation,
which is one of the most popular models for approximating scattered data in a
general number of dimensions \citep{buhmann2000radial,fasshauer2007meshfree,
schaback2006kernel,wendland2004scattered}.
Given distinct interpolation points $\Theta=\{\theta^i\}_{i=1}^n$, the RBF model
takes the form
\begin{equation}\label{eq:rbf}
  s_{\Theta}(\theta) = \sum_{i=1}^n \lambda_i \varphi(\norm{x - \theta^i}) + p
  (x)\,,
\end{equation}
where the kernel $\varphi\Colon\mathbb{R}_{\geq 0} \to \mathbb{R}$ is a
one\hyp{}dimensional function and $p \in \Pi_{m-1}^d$, the space of polynomials
with $d$ variables of degree no more than $m-1$. There are many possible choices
for $\varphi$, such as the cubic kernel $\varphi(r)=r^3$ that we adopt in our
experiments, and the thin-plate spline kernel $\varphi(r)=r^2\,\log(r)$. The
coefficients $\lambda_i$ are determined by imposing the interpolation conditions
$s_{\Theta}(\theta^i) = \log\abs{K(\theta^i)}$ for $i=1,\ldots,n$ and the
discrete orthogonality condition
\begin{equation}\label{eq:disc_orthg}
  \sum_{i=1}^n \lambda_i q(\theta^i) = 0, \qquad \forall q \in \Pi_{m-1}^d.
\end{equation}
For appropriate RBF kernels, this linear system is nonsingular provided that
polynomials in $\Pi_{m-1}^d$ are uniquely determined by their values on the
interpolation set.