This dissertation is about computational tools based on randomized numerical
linear algebra for handling large-scale matrix data. Since large datasets become
commonly available in a wide variety of modern applications, there has been an
increasing demand of numerical methods for storing, processing, and learning
from them. Matrices, the classical form for representing datasets, naturally
connect these tasks with the rich literature of numerical linear algebra. In the
past few decades, scalability has emerged as a central theme of the field, as
the growth of data starts to surpass the development in computation power. For a
diverse collection of problems, randomized methods offer extraordinary
efficiency and scalability, by introducing controlled stochasticity according to
classical perturbation theory. This work focuses on using randomized
numerical linear algebra to build practical algorithms for problems of huge size
and high complexity otherwise. Through this dissertation, we explore topics
across network science, Gaussian processes regression, natural language
processing, and quantum chemistry.

\cref{ch2} studies the spectral densities of massive real-world graphs. We
borrow tools developed in condensed matter physics, and add novel adaptations to
handle the spectral signatures of common graph motifs.  The resulting methods
are highly efficient, as we illustrate by computing spectral densities for
graphs with over a billion edges on a single compute node. Beyond providing
visually compelling fingerprints of graphs, we show how the  estimation of
spectral densities facilitates the computation of many common centrality
measures, and use spectral densities to estimate meaningful information about
graph structure that cannot be inferred from the extremal eigenpairs alone.

\cref{ch3,ch4} develop novel approaches for Gaussian processes (GPs) regression.
The computational bottleneck of kernel learning for GPs is computing the log
determinant and its derivatives of an $n\Times n$ kernel matrix. Building on
existing fast matrix-vector-multiplication approximation for kernel
matrices, we combine iterative methods with stochastic estimation to
lower the cost from $\calO(n^3)$ to $\calO(n)$. The resulting methods are highly
efficient and flexible, allowing us to work with datasets much larger than
the traditional GP capability. Furthermore, we extend these ideas to GP
regression on both function values and derivatives. Our approaches, together
with dimensionality reduction and preconditioning, let us scale Bayesian
optimization with derivatives to high-dimensional problems and large evaluation
budgets.

\cref{ch5} proposes a complete pipeline for spectral inference of topic
models that scales gracefully with both the size of vocabulary and the dimension
of latent space. It allows us to simultaneously compress and rectify the 
co\hyp{}occurrence statistics, then learn latent variables directly from the
compressed form without losing visible precision. We verify that our
methods perform comparably to previous approaches on both textual and
non-textual data.

\cref{ch6} describes how to use centroid Voronoi tessellation to accelerate
electronic structure calculation. The recently developed interpolative separable
density fitting decomposition compresses the redundant information in electron
orbital pairs through a set of non-uniform interpolation points. Our method,
implemented as a weight K-means algorithm with random initialization, achieves
comparable accuracy to the existing procedure but at a cost negligible in the
overall calculations. We also find that our algorithm as a continuation method
enhances the smoothness of the potential energy surface.