Today massive datasets appear in countless applications across scientific
fields. Empowered by modern computational infrastructure, researchers have spent
tremendous effort to take advantage of the abundance of information. Machine
learning, as one of the fields that rely heavily on large data, has
witnessed incredible development and had far\hyp{}reaching impact on nearly
every area of scientific discovery. Even in traditional fields, such as partial
differential equations, researchers are working on larger and more complex
systems, resulting in bigger datasets. Typically, matrices arise as the natural
representation of data in both explicit and implicit ways. Object\hyp{}feature
matrices are the most common type; second\hyp{}order models, such as
co\hyp{}occurrence matrices and kernel matrices, capture the relationship
between pairs of objects; Laplacian matrices and other linear operators describe
dynamics on objects. In most instances, the matrices grow in size with the
datasets; thus, manipulating them becomes an inevitable challenge. Numerical
linear algebra (NLA), the study of matrix\hyp{}based numerical methods, promptly
becomes the driving force behind many of these applications.

Similarly, this ongoing trend of big data has led NLA into new and exciting
directions. While deterministic, factorization\hyp{}based methods usually have
strong theoretical guarantees and highly optimized implementations, emerging
problems have far exceeded the scale they are designed for, even with 
present\hyp{}day computational power. As a result, a great number of novel
algorithms centered around scalability have been developed in response to this
increasing demand. Randomized methods provide one particular approach that
brings efficiency through introducing stochasticity. Randomized NLA aims at
building fast algorithms that return sufficiently good approximate solutions.
Both ``fast'' and ``good'' are measured relatively depending on the underlying
problem, but the results are usually supported by matrix perturbation
theory and probability theory. Even though scalability is the foremost
objective, randomized NLA also designs algorithms around other criteria, e.g.,
interpretability, and robustness. \citet{drineas2016randnla} provide a great
overview of recent developments in randomized NLA.

This dissertation applies randomized NLA to large-scale data matrix in two
settings:
\begin{enumerate}
	\item The matrix is data sparse, i.e., it can be described with far fewer
	than $\calO(NM)$ parameters, where $N\times M$ is its size. The goal is to
	efficiently obtain a compressed representation of the matrix, ideally in an
	interpretable way.
	\item Only partial information from the matrix is required. The matrix
	may be explicitly available, but the extraction of information is expensive;
	or the matrix is implicit, due to the cost of formation or storage.
\end{enumerate}

These two scenarios are not mutually exclusive. For example, \cref{ch4} covers
the computation of the log\hyp{}determinant for kernel matrices, where we can
often take advantage of their low\hyp{}rank plus diagonal structure. The
applications we work with come from network science, Gaussian process
regression, natural language processing, and quantum chemistry. The diversity in
background alone is a compelling evidence for the versatility of randomized NLA.
The outline is detailed below.

\cref{ch2} covers some mathematical preliminaries as well as common topics
shared between the following chapters.

\cref{ch3} studies the spectral densities of massive real\hyp{}world graphs. We
borrow tools developed in condensed matter physics, and add novel adaptations to
handle the spectral signatures of common graph motifs.  The resulting methods
are highly efficient, as we illustrate by computing spectral densities for
graphs with over a billion edges on a single compute node. Beyond providing
visually compelling fingerprints of graphs, we show how the estimation of
spectral densities facilitates the computation of many common centrality
measures, and use spectral densities to estimate meaningful information about
graph structure that cannot be inferred from the extremal eigenpairs alone. This
work is accepted for publication in KDD 2019~\cite{dong2019network}.

\cref{ch4} develops novel approaches for Gaussian process (GP) regression.
The computational bottleneck of kernel learning for GPs is computing the log
determinant and its derivatives of an $N\Times N$ kernel matrix. Building on
existing fast matrix-vector-multiplication approximation for kernel
matrices, we combine iterative methods with stochastic estimation to
lower the cost from $\calO(N^3)$ to $\calO(N)$. The resulting methods are highly
efficient and flexible, allowing us to work with datasets much larger than
the traditional GP capability. Furthermore, we extend these ideas to GP
regression on both function values and derivatives. Our approaches, together
with dimensionality reduction and preconditioning, let us scale Bayesian
optimization with derivatives to high\hyp{}dimensional problems and large
evaluation budgets. This work appeared in NeurIPS 2017~\cite{dong2017scalable}
and NeurIPS 2018~\cite{eriksson2018scaling}.

\cref{ch5} proposes a complete pipeline for spectral inference of topic
models that scales well with both the size of the vocabulary and the
dimension of the latent space. It allows us to simultaneously compress and
rectify the co\hyp{}occurrence statistics, then learn latent variables directly
from the compressed form with little loss of precision. We verify that our
methods are as accurate as previous approaches on both textual and
non\hyp{}textual data, and run much faster. The work is in submission.

\cref{ch6} describes how to use centroid Voronoi tessellation to accelerate
electronic structure calculation. The recently\hyp{}developed interpolative
separable density fitting decomposition compresses the redundant information in
electron orbital pairs through a set of non\hyp{}uniform interpolation points.
Our method, implemented as a weighted K-means algorithm with random
initialization, achieves comparable accuracy to the existing procedure but at a
cost negligible in the overall calculations. We also find that our algorithm, as
a continuation method, enhances the smoothness of the potential energy surface.
This work appeared in Journal of Chemical Theory and Computation~
\cite{dong2018interpolative}.